
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>GES-X</title>

    <link href="./css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Kaushan+Script|Droid+Serif:400,700,400italic,700italic|Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/solid.css" integrity="sha384-VGP9aw4WtGH/uPAOseYxZ+Vz/vaTb1ehm1bwx92Fm8dTrE+3boLfF1SpAtB1z7HW" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/fontawesome.css" integrity="sha384-1rquJLNOM3ijoueaaeS5m+McXPJCGdr5HcA03/VHXxcp2kX2sUrQDmFc3jR5i/C7" crossorigin="anonymous">
	<link rel="icon" href="./images/GES-X_logo.PNG?version=1" type="image/x-icon">

    <!-- Custom styles for this template -->
    <link href="./css/agency.css" rel="stylesheet">
  </head>

  <body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav", style = "background-color: rgba(159, 193, 213, 0.779)">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top"> <img style="width:8em;" src="./images/GES-X_logo_white.PNG" alt="Logo"></a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <i class="fa fa-bars fa-2x" aria-hidden="true"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item" style="font-size: 110%">
              <a class="nav-link js-scroll-trigger" href="#dataset">Dataset</a>
            </li>
            <li class="nav-item" style="font-size: 110%">
              <a class="nav-link js-scroll-trigger" href="#model">Model</a>
            </li>
			      <li class="nav-item" style="font-size: 110%">
              <a class="nav-link js-scroll-trigger" href="#experiments">experiments</a>
            </li>

          </ul>
        </div>
      </div>
    </nav>


    <!-- Header -->
    <header class="masthead">
      <div class="container">
            <div class="row align-items-center intro-text">
              <div class="intro-lead-in">GES-X: a Large-scale 3D Co-speech Gesture Dataset</div>
              <div class="w-20">
                  <p>We collect a large-scale 3D meshed co-speech whole-body dataset that contains more 
                    than 40M posture instances across about 4.3K aligned speaker audios, dubbed <b>GES-X</b>. To the
                    best of our knowledge, this is the largest whole-body meshed 3D co-speech gesture dataset, whose 
                    duration is 15 times of the next largest one.</p >
              </div>
              <div class="w-80">
                <img style="float: right; width: 90%" src="./images/pie.png" alt="">
              </div>
        </div>
      </div>
    </header>

<!-- Dataset -->
<section class="bg-transparent" id="dataset">
  <div class="container">

    <div class="row">
      <!-- news column -->
      <div class="col-md-7">
        <h4 class="service-heading">Basic informations</h4>
        <ul class="text-muted">
          <li class="text-muted">Videos are collected from talk show videos.</li>
          <li class="text-muted"><b>4.3K videos</b> in total, including both standing and sitting positions.</li>
          <li class="text-muted">Integrates a wide range of attributes, including facial, mesh, phoneme, text, body, hand, and joint annotations.</li>
        </ul>
      </div>
      <!-- characteristics column -->
      <div class="col-md-5">
        <h4 class="service-heading">Basic statistics</h4>
        <ul class="text-muted">
          <li class="text-muted">Total duration of videos is <b>450 hours</b>.</li>
          <li class="text-muted">Average duration of videos is <b>13min</b>.</li>
          <li class="text-muted">More than <b>40M</b> gesture frames.</li>
          <li class="text-muted"><b>80,720</b> word corpus in total.</li>
        </ul>
      </div>

    <!-- video banner row -->
        <div class="row text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-top:40px">Examples</h4>
      </div>
      <p style="text-align: left; margin-left: 15px; margin-bottom: 20px; color: rgb(171, 171, 171)">
        Some video-pose examples in GES-X dataset. Original video clips are shown on the left, and the corresponding pose sequences are shown on the right.
      </p>
	   <div class="col-md-12" style="text-align:left">
       <table>
          <tr>
            <td style="width:3%"></td>
            <td style="width:50%">
              <iframe width="614" height="207" src="./videos/-3f9zdKh7Oc.mp4" title="video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 50%">
              <iframe width="614" height="207" src="./videos/2LTe9T2zwEM.mp4" title="video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

          </tr>
          <tr>
            <td style="width:1%"></td>
            <td style="width: 50%">
              <div style="text-align:left; overflow-x:auto ;overflow-y: auto;;margin-bottom: 30px; margin-top: 10px">
                <font color="#4682B4">
                  <b> The information of the audible segments:</b><br>
                </font>                
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 50%">
              <div style="text-align:left; overflow-x:auto ;overflow-y: auto;margin-bottom: 30px; margin-top: 10px">
                <font color="#4682B4">
                  <b> The information of the audible segments:</b><br>
                </font> 
              </div>
            </td>
          </tr>
          <tr>
            <td style="width:3%"></td>
            <td style="width:50%">
              <iframe width="614" height="207" src="./videos/-3f9zdKh7Oc.mp4" title="video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 50%">
              <iframe width="614" height="207" src="./videos/2LTe9T2zwEM.mp4" title="video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

          </tr>
          <tr>
            <td style="width:1%"></td>
            <td style="width: 50%">
              <div style="text-align:left; overflow-x:auto ;overflow-y: auto;margin-bottom: 30px; margin-top: 10px">
                <font color="#4682B4">
                  <b> The information of the audible segments:</b><br>
				  (1) Female singing:</br>
                </font>                
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 50%">
              <div style="text-align:left; overflow-x:auto ;overflow-y: auto;margin-bottom: 30px; margin-top: 10px">
                <font color="#4682B4">
                  <b> The information of the audible segments:</b><br>
                </font> 
              </div>
            </td>
          </tr>

        </table>
  </br>
  </div>
</div>


  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center" style="margin-bottom: -30px">
        <h2 class="section-heading text-uppercase" style="color: rgb(90, 90, 90); margin-top: 90px">GES-X Statistics</h2>
        <h3 class="section-subheading" style="color: rgb(171, 171, 171)">Dataset Analysis and Statistics</h3>
      </div>
    </div>
	<h5 class="section-subheading" style="text-align:left; color: rgb(100, 100, 100); margin-top: 20px"> Overview of GES-X Dataset:</h5>
	    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="./images/dataset_table.jpg" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left; margin-left:15px"> 
		Statistics comparison of our GES-X with existing ones. The dotted line “- - -” separates
    whether the posture in the dataset is built based on the mesh. As for the duration of meshed whole body
    co-speech gesture datasets, our GES-X is 15 times of the second best one (i.e.BEAT2).
      </p>
    </div>
	
	</br>
	<h5 class="section-subheading" style="text-align:left; margin-top:20px; color: rgb(100, 100, 100)"> Dataset statistical comparison:</h5>
	</br>
  <div class="row">
    <!-- news column -->
    <div class="col-md-6">
      <div class="col-md centered" style="margin-top: 28px; margin-left: -20px">
        <img src="./images/word_corpus.svg" style="width: 100%" class="img-responsive"/> 
      </div>
    </div>
    <!-- characteristics column -->
    <div class="col-md-5">
      <div class="col-md centered" style="padding:1rem;">
        <img src="./images/motion_degree.svg" style="width: 90%" class="img-responsive"/> 
      </div>
    </div>
    <p class="text-muted" style="text-align:left; margin-left:15px"> 
      Dataset statistical comparison between our
      GES-X and existing meshed co-speech gesture datasets
      (i.e.BEAT2, TalkSHOW). Our GES-X has a
      much larger word corpus and a more widely uniform
      distributed gesture motion degree.
    </p>
	  </div>
    </div>
  </div>
</section>

<!-- Model -->
<section id="model">
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center" style="margin-bottom: -30px">
        <h2 class="section-heading" style="color: rgb(90, 90, 90); margin-top: 30px">CoCoGesture</h2>
        <h3 class="section-subheading" style="color: rgb(171, 171, 171)">Our framework CoCoGesture and key module Mixture-of-Gesture-Experts (MoGE) block. </h3>
      </div>
    </div>

  <h5 class="section-subheading" style="text-align:left; color: rgb(100, 100, 100); margin-top: 20px">Task Introduction:</h5>
    <div class="row text-center">
      <p class="text-muted" style="text-align:left; margin-left:15px"> 
        In our work, we focus on the task of vivid and diverse co-speech 3D gesture generation from in-the-wild human voices.<br> Details about our task are shown in the figure below:
      </p>
    </div>
    <div class="col-md centered" style="padding:1rem; margin-top: 10px; ">
      <img src="./images/teaser.svg" style="width: 100%" class="img-responsive"/> 
    </div>


</br>
	<h5 class="section-subheading" style="text-align:left; margin-top:20px; color: rgb(100, 100, 100)">CoCoGesture Framework:</h5>
	    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem; margin-top:20px; ">
        <img src="./images/pipeline.svg" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left; margin-left:15px"> 
        Along with GES-X, we proposed <b>CoCoGesture</b>, a novel framework enabling vivid and diverse
        gesture synthesis from unseen voice of human speech prompts. Our key insight
        is built upon the custom-designed pretrain-fintune training paradigm.
      </p>
    </div>
	
	</br>
	<h5 class="section-subheading" style="text-align:left; margin-top:20px; color: rgb(100, 100, 100)">Mixture-of-Gesture-Experts (MoGE):</h5>
	<div class="row justify-content-md-center text-center">
    <div class="col-md centered" style="padding:1rem; margin-top:20px; ">
      <img src="./images/MoGE.svg" style="width: 60%" class="img-responsive"/> 
    </div>
    <p class="text-muted" style="text-align:left; margin-left:15px"> 
      Moreover, we designed a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio 
      embedding from the human speech and the gesture features from the pre-trained 
      gesture experts with a routing mechanism. <br>
      Such an effective manner ensures 
      audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. 
  </div>
	  </div>
	
    

</section>


<!-- Experiments -->
 <section id="experiments">
  <div class="container">
 
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Strong Baselines for Benchmarking</h2>
        <h3 class="section-subheading text-muted">Different Tasks Adopted to LU-AVS, A Strong Baseline for LU-AVS, Experimental Results and Challenge Analysis</h3>
      </div>
    </div>
	
	<div class="row">
      <div class="col-md-12">
        <p class="text-muted" style="text-align:left">
		To show the necessity and fully explore the challenges of LU-AVS, we investigate the performance of existing <b>audio-visual segmentation (AVS)</b>, 
		<b>audio-visual localization (AVL)</b>, 
		<b>audio-visual event localization (AVE)</b>, and <b>spatio-temporal video grounding (STVG)</b> methods on our LU-AVS dataset. 
		Based on the analysis of the above methods, we introduce a <b>simple yet effective framework</b> to provide a base reference for long video audible object grounding in the future.
		</p>
        <div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
        <img src="./images/task.png" style="width: 90%;" class="img-responsive"/> 
        </div> 
      </div>
    </div>
     <br/>
    <div class="row">
      <div class="col-md-12">
        <p class="text-muted" style="text-align:left">
        <p class="text-muted" style="text-align:left">
          <b>More details are in the <a href="#">[Paper]</a> and <a href="#">[Supplementary]</a></b>.<br/>  
        </p> 
      </div>
    </div>
    
    <h5 class="section-subheading" style="text-align:left; color: rgb(100, 100, 100)"> Comparison with the state-of-the-art counterparts:</h5>
    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="./images/data_metrics.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left">
        Comparison with the state-of-the-art counterparts on BEAT2 and TalkSHOW datasets. ↑ means the higher the better, and ↓ indicates the lower the better. “-” denotes that the method cannot be applied to the TalkSHOW dataset due to the lack of
        text transcripts. The term “zero-shot’‘ implies that the dataset contains unseen human voices.
    </div>

   <div class="row">
		<div class="col-md-12">
			<h5 class="subheading">Task Definition:</h5>
			<br/>
			<h6 class="subheading">Audio-Visual Segmentation Methods (AVS):</h6>
			<p class="text-muted" style="text-align:left">
			<b>The goal of the AVS task is to segment audible objects in an image based on a given audio-visual pair.</b>
			Existing methods are designed based oncdatasets with the fixed input format of 10 frames corresponding to 10s audio. 
			To adapt these methods for untrimmed videos, we modify the input to one second of audio and five uniformly sampled frames from the segment, allowing audio-visual interactions within the per-second segment.
		</p>
		</div>
		
		<div class="col-md-12">
			<h6 class="subheading">Audio-Visual Localization Methods (AVL):</h6>
			<p class="text-muted" style="text-align:left">
			<b>The AVL task also focuses on locating audible objects in the spatial dimension.</b>
			However, AVL presents sounding regions by heatmaps.
			For comparison, in the test stage, we convert the heatmaps into bounding boxes as other works. Thanks to our bounding-box annotations, we can evaluate AVL on our LU-AVS dataset in a unified manner.
			Similar to the modification for AVS methods, we slice the videos into 1-second segments to fit the AVL methods.
		</div>
		
		<div class="col-md-12">
			<h6 class="subheading">Audio-Visual Event Localization Methods (AVE):</h6>
			<p class="text-muted" style="text-align:left">
			<b>AVE task aims at determining the audio-visual temporal segments when the target object is both audible and visible at the same time. </b>
			This task does not focus on segmenting audible objects on the spatial dimension, they cannot be applied to segmenting objects in images.
			These methods are also developed based on the videos with fixed durations.
		</div>
		
		<div class="col-md-12">
			<h6 class="subheading">Spatio-temporal Video Grounding Methods (STVG):</h6>
			<p class="text-muted" style="text-align:left">
			<b>Given a query sentence, STVG methods are required to track the target object in the video both at the spatial (bounding boxes) and temporal dimensions (start and end time positions). </b>
			Unlike the AVS task that may require segmenting multi-sounding objects in a video, the STVG methods only need to find and track one target object for each text-video pair.
			To explore the performance of STVG methods on LU-AVS, we modify these text-guided methods to adapt to our task.
		</div>
		
		<div class="col-md-12">
			<h6 class="subheading">A Strong LU-AVS Baseline:</h6>
			<div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
			<img src="./images/pipeline.png" style="width:70%;" class="img-responsive"/> 
			</div> 
			<p class="text-muted" style="text-align:left">
			<b>The overview architecture of a strong baseline. </b>It first learns visual and audio features separately and then establishes visual and audio associations. 
			It enables us to dissect the impacts of visual and audio branches explicitly.
			</p>
			</br>
			<p class="text-muted" style="text-align:left">
			Different from previous AVS datasets, the LU-AVS dataset introduces unique challenges of detecting and segmenting audible objects in long videos.
			In untrimmed videos, the target objects may emit or stop sound at a random position in a video, and the sounding durations of objects are various 
			in the dataset. To achieve the spatial and temporal localization of audible objects, 
			we introduce a simple framework to provide a base reference for long video audible object grounding in the future.
			</p>
</div>
    </div>

    <br/>
    <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Benchmarking Results and Analysis</h4>
        <p class="text-muted" style="text-align:left">
          <b>Benchmarking results on the LU-AVS dataset. </b> For all the evaluation metrics, higher values indicate better performance.
		  Notably, in AVL methods, the spatial localization results are presented by heatmaps. 
		  For comparison, we convert heatmaps to bounding boxes as exsiting works.
		  Additionally, AVE methods focus on temporal localization. 
		  Here, m_tIoU represents the segmentation accuracy within the ground-truth temporal range, 
		  and m_vIoU indicates the segmentation accuracy over the temporal union between the predicted and ground-truth durations.
		  Besides, we replace the text branch in the STVG methods with an audio branch for spatial-temporal audible object grounding.
        </p>

        <div class="col-md centered" style="padding:0.6rem; text-align:center">
        <img src="./images/exp_results.jpg" style="width: 90%;" class="img-responsive"/> 
        </div>
      </div>
    </div>

    <br/>

    
    <div class="row">
      <div class="col-md-12">
       <h5 class="subheading">Challenges Imposed by LU-AVS Dataset</h5>                    
        <p class="text-muted">        
		Based on the above experimental results, we summarize the dataset challenges and adaptability of existing methods as follows: </br></br>
        (1) <b>For long videos in LU-AVS, the sounding duration and the start- and end-sounding time positions are uncertain. </b></br>
		Therefore, both the spatial and temporal localization of audible objects are necessary for LU-AVS. 
		Existing AVS methods developed based on the trimmed videos struggle to achieve temporal localization, showing limited adaptability in long videos. <br/> 
        </p>
 
		<p class="text-muted">        
        (2) <b> Unlike trimmed videos always feature audible objects, untrimmed videos contain a high proportion of silent segments.  </b></</br>
		Hence, the existing AVL methods trained on LU-AVS tend to overlook the audible objects.
		This suggests the requirement for greater emphasis on audio in model development on LU-AVS.<br/>        
        </p>
		
 		<p class="text-muted">        
        (3) <b>Similar to the STVG task, the exhaustive annotations in LU-AVS pose a high demand for achieving consistent spatial and temporal localization of audible objects, 
		requiring methods to effectively joint model spatial, temporal, and audio-visual interactions.</b><br/>        
        </p>
		<br/>
		</div>
	</div>
	</br></br>

<section class="bg-transparent" id="downloads">
  <div class="container">
    
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Download</h2>
        <h3 class="section-subheading text-muted">Dataset publicly available for research purposes</h3>
      </div>
    </div>


    <div class="row">
      <div class="col-md-12"> 
        
        <h4 class="section-subheading" id="downloadFiles">Data download </h4><hr/>
        
        <p><b>Resources (Anonymous Now):</b>
          <ul>
            <li>All untrimmed videos: &nbsp;&nbsp; Download from&nbsp;
              <a href="#">Google Drive</a>,&nbsp;</li>
            <li>Mask Annotations: &nbsp;&nbsp; Download from&nbsp;
               <a href="#">Google Drive</a>,&nbsp;</li>
            <li>Bounding Box Annotations: &nbsp;&nbsp;, Download from&nbsp;
			<a href="#">Google Drive</a>,&nbsp;</li>
            </ul>
        </p>

        <p> <b>Annotations</b> (train, val and test set): Available for download at <a href="#">GitHub  (Anonymous Now):</a></p>
		<p> The format of a JSON file is shown in below:</p>
       <pre class="hidden-sm hidden-md hidden-lg">
<em class="comment">Please view the JSON format in larger screen</em>
</pre>

<pre class="hidden-xs">
{
    "version": "VERSION 1.0",
	{
		"video_id": "0YMv3RUxGQM",                       <em class="comment", style="color:#ff6600"># Video ID in LU-AVS</em>
		"video_path": "./0YMv3RUxGQM.mp4",               <em class="comment", style="color:#ff6600"># Relative path name in this dataset</em>
		"frame_count": 635,
		"fps": 29.71, 
		"width": 406, 
		"height": 720, 
		"subject_objects": [                            <em class="comment", style="color:#ff6600"># The ID of the audible segment in this video</em>
			"0": {
				"category_name": "engine_knocking",     <em class="comment", style="color:#ff6600"># The category name of an audible segment</em>
				"start_frame": 212,
				"end_frame": 632,
				"mask_dir": "./0YMv3RUxGQM__engine_knocking__ST212__ET632", <em class="comment", style="color:#ff6600"># The directory with annotated masks of the video</em>
				"trajectory_bbox_path": "./0YMv3RUxGQM.json"       <em class="comment", style="color:#ff6600"># Relative annotation recording path of a video</em>
			}, 
			...
		], 
	}
	...
}
</pre>

<em class="comment">The annotated json file of audible segments in a video (0YMv3RUxGQM.json).</em>

<pre class="hidden-xs">
 
{
	"0": {                                          <em class="comment", style="color:#ff6600"># The ID of the audible segment in this video</em>       
		"212"{                                  <em class="comment", style="color:#ff6600"># The frame ID of the audible segment</em>
			"bbox": [
				161,                            <em class="comment", style="color:#ff6600"># Left</em>
				34,	                        <em class="comment", style="color:#ff6600"># Top</em>
				1028,                           <em class="comment", style="color:#ff6600"># Right</em>
				685                             <em class="comment", style="color:#ff6600"># Bottom</em>
			]
			category_name: "engine_knocking",    <em class="comment", style="color:#ff6600"># The trajectory ID to which the bounding box belongs</em>
			mask_path: "0/000212.png,           <em class="comment", style="color:#ff6600"># The specific mask path of the frame</em>
			generated: 0,                       <em class="comment", style="color:#ff6600"># 1 - the bounding box is automatically generated by a tracker</em>
						           <em class="comment", style="color:#ff6600"># 0 - the bounding box is manually labeled</em>
		}, 
		...
	},
	...
}
</pre>
        
        <br/>
        <h4 class="section-subheading">Publication(s)</h4>
        <p>
          If you find our work useful in your research, please cite our paper.
        </p>
        <pre class="bibtex" style="text-align:left; margin-left:2px">
        <code>
        @ARTICLE{
          title={Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos},
          author={anonymous},
          year={2024},
        }</code>
        </pre>


        <br/>
        <h4 class="section-subheading">Disclaimer </h4>
        <p> The released LU-AVS dataset is curated, which perhaps owns potential correlation between instrument and geographical area. This issue warrants further research and consideration.
        </p>

      </div>
    </div>


    <br/>
    <div class="row">
      <div class="col-md-12">
        <h4 class="section-subheading">Copyright <img alt="Creative Commons License" style="border-width:1px;float:left;margin-right:15px;margin-bottom:0px;" src="https://i.creativecommons.org/l/by-nc/3.0/88x31.png"/></h4>
        <p>
          All datasets and benchmarks on this page are copyright by us and published under the <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International</a> License. This means that  you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. You may not use the material for commercial purposes.
        </p>
      </div>
    </div>
  </div>
</section>

    <div class="container">
      <div class="row">
      </div>
    </div>
</section>



<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"LU-AVS dataset",
  "description":"First-person (egocentric) video dataset; multi-faceted non-scripted recordings in the wearers' homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel live audio commentary approach.",
  "url":"https://github.com/epic-kitchens/annotations",
  "sameAs":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d",
  "citation":"Damen, Dima et al. 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset', European Conference on Computer Vision, 2018",
  "identifier": "10.5523/bris.3h91syskeag572hl6tvuovwv4d",
  "keywords":[
     "Egocentric vision",
     "Human actions",
     "Object interactions",
     "actions",
     "video",
     "kitchens",
     "cooking",
     "dataset",
     "epic kitchens",
     "epic",
     "eccv",
     "2022"
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://epic-kitchens.github.io/",
     "name":"EPIC Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"uob-epic-kitchens@bristol.ac.uk",
        "url":"https://github.com/epic-kitchens/annotations/issues"
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     }
  ],
  "license": "https://creativecommons.org/licenses/by-nc/4.0/"
}
</script>

    
    <!-- Footer -->
    <footer style="background-color:#373435ff;">
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <img alt="Creative Commons License" style="border-width:1px;float:left;margin-right:15px;margin-bottom:0px;" src="http://i.creativecommons.org/l/by-nc/3.0/88x31.png"/>
            <span class="copyright" style="color:#eee;">Copyright &copy;  (Anonymous Now):</span>
          </div>
          <div class="col-md-8">
            <p style="color:#eee;">For any questions, email us at
              <a href="mailto:#"> anonymous</a></p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="./jquery/jquery.min.js"></script>
    <script src="./jquery//bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./jquery/jquery.easing.min.js"></script>
    <!-- Custom scripts for this template -->
    <script src="./jquery/agency.min.js"></script>

  </body>
</html>
